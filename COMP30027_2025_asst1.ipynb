{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2025 Semester 1\n",
    "\n",
    "## Assignment 1: Scam detection with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     `1389444`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, GRAPHS, AND FIGURES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).** Results, figures, etc. which appear in this file but are NOT included in your report will not be marked.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probability of scam, P(scam) = 0.2000\n",
      "Prior probability of non-malicious, P(non-malicious) = 0.8000\n",
      "\n",
      "Most probable words in scam class:\n",
      ".: 0.0565\n",
      "!: 0.0243\n",
      ",: 0.0235\n",
      "call: 0.0205\n",
      "£: 0.0139\n",
      "free: 0.0105\n",
      "/: 0.0091\n",
      "2: 0.0088\n",
      "&: 0.0087\n",
      "?: 0.0085\n",
      "\n",
      "Most probable words in non-malicious class:\n",
      ".: 0.0793\n",
      ",: 0.0260\n",
      "?: 0.0256\n",
      "u: 0.0189\n",
      "...: 0.0187\n",
      "!: 0.0172\n",
      "..: 0.0149\n",
      ";: 0.0132\n",
      "&: 0.0131\n",
      "go: 0.0111\n",
      "\n",
      "Most predictive words for non-malicious class:\n",
      ";: 60.50\n",
      "...: 57.50\n",
      "gt: 54.06\n",
      "lt: 53.55\n",
      ":): 47.88\n",
      "ü: 31.92\n",
      "lor: 28.83\n",
      "ok: 24.71\n",
      "hope: 24.71\n",
      "d: 21.11\n",
      "\n",
      "Most predictive words for scam class:\n",
      "prize: 0.01\n",
      "tone: 0.02\n",
      "£: 0.02\n",
      "select: 0.02\n",
      "claim: 0.02\n",
      "paytm: 0.03\n",
      "code: 0.03\n",
      "award: 0.03\n",
      "won: 0.03\n",
      "18: 0.03\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load supervised train dataset\n",
    "sv_train = pd.read_csv(\"sms_supervised_train.csv\")\n",
    "sv_train[\"textPreprocessed\"] = sv_train[\"textPreprocessed\"].astype(str)\n",
    "\n",
    "texts = sv_train[\"textPreprocessed\"] # assign the textPreprocessed column as a separate variable\n",
    "labels = sv_train[\"class\"] # assign the class column as a separate variable\n",
    "\n",
    "# Compute prior probabilities P(c)\n",
    "N_total = len(sv_train) # Dataset size\n",
    "N_scam = 0  # Scam label count\n",
    "N_nonmal = 0 # Non-malicious label count\n",
    "\n",
    "# Iterate and count the frequency of each label\n",
    "for label in labels:\n",
    "    if label == 0:\n",
    "        N_nonmal += 1\n",
    "    else:\n",
    "        N_scam += 1\n",
    "\n",
    "P_scam = N_scam / N_total   # Prior probability of scam instances\n",
    "P_nonmal = N_nonmal / N_total   # Prior probability of non-malicious instances\n",
    "prior_prob = {0: P_nonmal, 1: P_scam}\n",
    "\n",
    "# Display P(c) results\n",
    "print(f\"Prior probability of scam, P(scam) = {P_scam:.4f}\")\n",
    "print(f\"Prior probability of non-malicious, P(non-malicious) = {P_nonmal:.4f}\\n\")\n",
    "\n",
    "word_count_matrix = {0: defaultdict(int), 1: defaultdict(int)}  # Count matrix for each word separated by label\n",
    "vocabulary = set()  # List of every unique words in the dataset\n",
    "\n",
    "# Iterate and count the words frequency in each class while adding new words to the vocabulary\n",
    "for text, label in zip(texts, labels):\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        word_count_matrix[label][word] += 1\n",
    "        vocabulary.add(word)\n",
    "\n",
    "N_vocab = len(vocabulary)  # Vocabulary size\n",
    "alpha = 1  # Laplace smoothing factor\n",
    "\n",
    "# Compute word probabilities and store them as a matrix\n",
    "word_probability_matrix = {0: {}, 1: {}}\n",
    "for word in vocabulary:\n",
    "    word_probability_matrix[0][word] = (word_count_matrix[0][word] + alpha) / (sum(word_count_matrix[0].values()) + N_vocab * alpha)\n",
    "    word_probability_matrix[1][word] = (word_count_matrix[1][word] + alpha) / (sum(word_count_matrix[1].values()) + N_vocab * alpha)\n",
    "\n",
    "top_scam_words = sorted(word_probability_matrix[1].items(), key=lambda x: x[1], reverse=True)[:10]  # Top 10 most probable words for scam\n",
    "top_nonmal_words = sorted(word_probability_matrix[0].items(), key=lambda x: x[1], reverse=True)[:10] # Top 10 most probable words for non-malicious\n",
    "\n",
    "# Display the most probable words for each class\n",
    "print(\"Most probable words in scam class:\")\n",
    "for word, prob in top_scam_words:\n",
    "    print(f\"{word}: {prob:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Most probable words in non-malicious class:\")\n",
    "for word, prob in top_nonmal_words:\n",
    "    print(f\"{word}: {prob:.4f}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Compute predictive word ratios R (class 0 / class 1)\n",
    "ratios = {word: word_probability_matrix[0][word] / word_probability_matrix[1][word] \n",
    "          for word in vocabulary if word_probability_matrix[1][word] > 0}\n",
    "\n",
    "top_predictive_nonmal = sorted(ratios.items(), key=lambda x: x[1], reverse=True)[:10]  # Top 10 most predictive words for non-malicious\n",
    "top_predictive_scam = sorted(ratios.items(), key=lambda x: x[1])[:10]  # Top 10 predictive words for scam\n",
    "\n",
    "# Display the most predictive words for each class\n",
    "print(\"Most predictive words for non-malicious class:\")\n",
    "for word, ratio in top_predictive_nonmal:\n",
    "    print(f\"{word}: {ratio:.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"Most predictive words for scam class:\")\n",
    "for word, ratio in top_predictive_scam:\n",
    "    print(f\"{word}: {ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial, log, exp\n",
    "\n",
    "def posterior_count_class(word_counts, class_label, vocabulary, word_probability_matrix):\n",
    "    n = sum(word_counts.values())\n",
    "    \n",
    "    numerator = factorial(n)\n",
    "    denominator = 1\n",
    "    prob_product = 1.0\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        word_prob = word_probability_matrix[class_label][word]\n",
    "        denominator *= factorial(count)\n",
    "        prob_product *= word_prob ** count\n",
    "\n",
    "    return (numerator / denominator) * prob_product\n",
    "\n",
    "def log_posterior_count_class(word_counts, class_label, vocabulary, word_probability_matrix):\n",
    "    n = sum(word_counts.values())\n",
    "    \n",
    "    log_numerator = log(factorial(n))\n",
    "    log_denominator = 0.0\n",
    "    log_prob_sum = 0.0\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        word_prob = word_probability_matrix[class_label][word]\n",
    "        log_denominator += log(factorial(count))\n",
    "        log_prob_sum += count * log(word_prob)\n",
    "\n",
    "    return log_numerator - log_denominator + log_prob_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9750\n",
      "Confusion Matrix:\n",
      "[[785  15]\n",
      " [ 10 190]]\n",
      "Precision (Non-Malicious, Scam): [0.98742138 0.92682927]\n",
      "Recall (Non-Malicious, Scam): [0.98125 0.95   ]\n",
      "\n",
      "Total OOV words encountered: 1571\n",
      "Total skipped test instances: 0\n",
      "\n",
      "Examples of Scam Classified with High Confidence:\n",
      "R=0.00 | Text: . 4 + call £ - * holiday & urgent 18 t landline 150ppm cash cs await collection po box sae complimentary 10,000 ibiza\n",
      "R=0.00 | Text: . 3 4 + ! call : £ offer * holiday & urgent 18 t landline 150ppm cash cs await collection po box sae tenerife 10,000\n",
      "R=0.00 | Text: . . . , please order text call / : customer tone number [ [ service mobile ] ] colour colour thanks ringtone reference charge 4.50 arrive = red x49 09065989182\n",
      "R=0.00 | Text: . call £ £ guarantee won customer prize prize claim service 1000 yr 2000 representative cash 10am-7pm\n",
      "R=0.00 | Text: . . 2 free u + ! 1st / / wk wk txt tone gr8 hit 150p 16 poly 8007 8007 nokia nokia nokia tones polys\n",
      "\n",
      "Examples of Non-Malicious Classified with High Confidence:\n",
      "R=91349944516297203886253521825135853568.00 | Text: time : rs. transaction number & & & & & & & & & & ; ; ; ; ; ; ; ; ; ; lt lt lt lt lt # # # gt gt gt gt gt credit account reference decimal\n",
      "R=269038185856116170020844732416.00 | Text: ? ? ? ? .. .. u u u u , , ... ... ... ... say person yes ! f : hello hello hello o o wen knw knw girl girl mean @ \" \" \" \" t name name g g n d d d d d d lift bt real dat h girlfrnd girlfrnd moral\n",
      "R=31829245411852975530311680.00 | Text: . every & & & & & & ; ; ; ; ; ; lt lt lt # # # gt gt gt big hr\n",
      "R=603560373774131789824.00 | Text: , get like second half & & & & ; ; ; ; lt lt # # gt gt run though almost whole gram gram usually\n",
      "R=38221697184201940992.00 | Text: u , , lor ... ... ... ... food food eat den oso haha well depend mon n la wana okie okie cheap chinese gd ex\n",
      "\n",
      "Examples on the Decision Boundary (Uncertain Classification):\n",
      "R=1.02 | Text: . call dear\n",
      "R=1.04 | Text: . reply glad\n",
      "R=0.93 | Text: . . tell return re order\n",
      "R=0.93 | Text: ? ur * just alrite sam\n",
      "R=0.88 | Text: . . reply send person right ! code confirm sort bank acc\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# Load test dataset\n",
    "test = pd.read_csv(\"sms_test.csv\")\n",
    "test[\"textPreprocessed\"] = test[\"textPreprocessed\"].astype(str)\n",
    "\n",
    "test_texts = test[\"textPreprocessed\"]   # assign the textPreprocessed column as a separate variable\n",
    "test_labels = test[\"class\"] # assign the class column as a separate variable\n",
    "\n",
    "oov_word_count = 0  # Count of out-of-vocabulary words\n",
    "skipped_instances = 0  # Number of skipped test messages\n",
    "\n",
    "predictions = []    # list of predictions for test instances\n",
    "confidence_ratios = []  # list of confidence ratios for test instances\n",
    "log_prior_prob = {0: np.log(P_nonmal), 1: np.log(P_scam)}\n",
    "\n",
    "for test_text, test_label in zip(test_texts, test_labels):\n",
    "    test_words = test_text.split()\n",
    "    test_word_counts = defaultdict(int)\n",
    "    \n",
    "    # Compute word count vector, ignoring OOV words\n",
    "    for test_word in test_words:\n",
    "        if test_word in vocabulary:\n",
    "            test_word_counts[test_word] += 1\n",
    "    oov_word_count += len(test_words) - len(test_word_counts)\n",
    "\n",
    "    # Skip instances with no known words\n",
    "    if len(test_word_counts) == 0:\n",
    "        skipped_instances += 1\n",
    "        predictions.append(None)\n",
    "        confidence_ratios.append(None)\n",
    "        continue\n",
    "\n",
    "    log_posterior_class_count_0 = log_prior_prob[0] + log_posterior_count_class(test_word_counts, 0, vocabulary, word_probability_matrix)\n",
    "    log_posterior_class_count_1 = log_prior_prob[1] + log_posterior_count_class(test_word_counts, 1, vocabulary, word_probability_matrix)\n",
    "\n",
    "    prediction = 0 if log_posterior_class_count_0 > log_posterior_class_count_1 else 1\n",
    "    predictions.append(prediction)\n",
    "\n",
    "    # Compute confidence ratio\n",
    "    log_confidence_ratio = log_posterior_class_count_0 - log_posterior_class_count_1\n",
    "    confidence_ratio = np.exp(log_confidence_ratio)\n",
    "    confidence_ratios.append((test_text, prediction, confidence_ratio))\n",
    "\n",
    "# Compute performance metrics\n",
    "accuracy = accuracy_score(test_labels[:len(predictions)], predictions)\n",
    "conf_matrix = confusion_matrix(test_labels[:len(predictions)], predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels[:len(predictions)], predictions, average=None)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Precision (Non-Malicious, Scam): {precision}\")\n",
    "print(f\"Recall (Non-Malicious, Scam): {recall}\\n\")\n",
    "\n",
    "print(f\"Total OOV words encountered: {oov_word_count}\")\n",
    "print(f\"Total skipped test instances: {skipped_instances}\")\n",
    "\n",
    "# Extract examples\n",
    "high_conf_nonmal = sorted([x for x in confidence_ratios], key=lambda x:x[2], reverse=True)[:5]  # High confidence in non-malicious (class 0)\n",
    "high_conf_scam = sorted([x for x in confidence_ratios], key=lambda x:x[2])[:5]  # High confidence in scam (class 1)\n",
    "boundary_cases = sorted([x for x in confidence_ratios], key=lambda x:abs(np.log(x[2])))[:5]  # Somewhere in the middle (low confidence for both class)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"\\nExamples of Scam Classified with High Confidence:\")\n",
    "for text, label, R in high_conf_scam:\n",
    "    print(f\"R={R:.2f} | Text: {text}\")\n",
    "\n",
    "print(\"\\nExamples of Non-Malicious Classified with High Confidence:\")\n",
    "for text, label, R in high_conf_nonmal:\n",
    "    print(f\"R={R:.2f} | Text: {text}\")\n",
    "\n",
    "print(\"\\nExamples on the Decision Boundary (Uncertain Classification):\")\n",
    "for text, label, R in boundary_cases:\n",
    "    print(f\"R={R:.2f} | Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending the model with semi-supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load unlabelled dataset\n",
    "unlabelled_df = pd.read_csv(\"sms_unlabelled.csv\")\n",
    "unlabelled_df[\"textPreprocessed\"] = unlabelled_df[\"textPreprocessed\"].astype(str)\n",
    "\n",
    "unlabelled_texts = unlabelled_df[\"textPreprocessed\"]\n",
    "unlabelled_true_labels = unlabelled_df[\"class\"]\n",
    "\n",
    "# Use stratified sampling to sample 20% of the dataset for validation set\n",
    "unlabelled_train, unlabelled_validation, labels_train, labels_validation = train_test_split(\n",
    "    unlabelled_texts, unlabelled_true_labels, \n",
    "    test_size=0.2, \n",
    "    stratify=unlabelled_true_labels, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 200 random instances\n",
      "\n",
      "New expanded prior probabilies {0: 0.8, 1: 0.2}\n",
      "Accuracy: 0.9600\n",
      "Confusion Matrix:\n",
      "[[309  11]\n",
      " [  5  75]]\n",
      "Precision (Non-Malicious, Scam): [0.98407643 0.87209302]\n",
      "Recall (Non-Malicious, Scam): [0.965625 0.9375  ]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "random_200 = random.sample(list(zip(unlabelled_train, labels_train)), 200)\n",
    "\n",
    "# Extract texts and true labels from the selected instances\n",
    "selected_texts = [x[0] for x in random_200]\n",
    "selected_labels = [x[1] for x in random_200]\n",
    "\n",
    "# Combine with original training data from Q1\n",
    "expanded_texts = list(texts) + selected_texts\n",
    "expanded_labels = list(labels) + selected_labels\n",
    "\n",
    "# Rebuild vocabulary and count matrix\n",
    "expanded_vocabulary = set()\n",
    "expanded_word_count_matrix = {0: defaultdict(int), 1: defaultdict(int)}\n",
    "\n",
    "for text, label in zip(expanded_texts, expanded_labels):\n",
    "    for word in text.split():\n",
    "        expanded_word_count_matrix[label][word] += 1\n",
    "        expanded_vocabulary.add(word)\n",
    "\n",
    "expanded_N_vocab = len(expanded_vocabulary)\n",
    "expanded_word_probability_matrix = {0: {}, 1: {}}\n",
    "\n",
    "# Recompute word probabilities with Laplace smoothing\n",
    "for word in expanded_vocabulary:\n",
    "    for label in [0, 1]:\n",
    "        expanded_word_probability_matrix[label][word] = (\n",
    "            expanded_word_count_matrix[label][word] + alpha\n",
    "        ) / (sum(expanded_word_count_matrix[label].values()) + expanded_N_vocab * alpha)\n",
    "\n",
    "# Recompute prior probabilities and the log of them\n",
    "expanded_total = len(expanded_labels)\n",
    "expanded_N_scam = sum(1 for l in expanded_labels if l == 1)\n",
    "expanded_N_nonmal = expanded_total - expanded_N_scam\n",
    "\n",
    "expanded_prior_prob = {0: expanded_N_nonmal / expanded_total, 1: expanded_N_scam / expanded_total}\n",
    "expanded_log_prior_prob = {0: np.log(expanded_prior_prob[0]), 1: np.log(expanded_prior_prob[1])}\n",
    "\n",
    "print(\"Selecting 200 random instances\\n\")\n",
    "print(f\"New expanded prior probabilies {expanded_prior_prob}\")\n",
    "\n",
    "# Test on original test set\n",
    "validation_predictions = [] \n",
    "for validation_text in unlabelled_validation:\n",
    "    validation_word_counts = defaultdict(int)\n",
    "    for word in validation_text.split():\n",
    "        if word in expanded_vocabulary:\n",
    "            validation_word_counts[word] += 1\n",
    "\n",
    "    if len(validation_word_counts) == 0:\n",
    "        validation_predictions.append(None)\n",
    "        continue\n",
    "\n",
    "    log_0 = expanded_log_prior_prob[0] + log_posterior_count_class(validation_word_counts, 0, expanded_vocabulary, expanded_word_probability_matrix)\n",
    "    log_1 = expanded_log_prior_prob[1] + log_posterior_count_class(validation_word_counts, 1, expanded_vocabulary, expanded_word_probability_matrix)\n",
    "    prediction = 0 if log_0 > log_1 else 1\n",
    "    validation_predictions.append(prediction)\n",
    "\n",
    "# Clean predictions for scoring\n",
    "valid_updated_preds = [p for p in validation_predictions if p is not None]\n",
    "valid_test_labels = [l for p, l in zip(validation_predictions, labels_validation) if p is not None]\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(valid_test_labels, valid_updated_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(valid_test_labels, valid_updated_preds, average=None)\n",
    "conf_matrix = confusion_matrix(valid_test_labels, valid_updated_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Precision (Non-Malicious, Scam): {precision}\")\n",
    "print(f\"Recall (Non-Malicious, Scam): {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 200 most uncertain instances\n",
      "\n",
      "New expanded prior probabilies {0: 0.8045454545454546, 1: 0.19545454545454546}\n",
      "Accuracy: 0.9675\n",
      "Confusion Matrix:\n",
      "[[311   9]\n",
      " [  4  76]]\n",
      "Precision (Non-Malicious, Scam): [0.98730159 0.89411765]\n",
      "Recall (Non-Malicious, Scam): [0.971875 0.95    ]\n"
     ]
    }
   ],
   "source": [
    "uncertain_instances = []\n",
    "\n",
    "for text, true_label in zip(unlabelled_train, labels_train):\n",
    "    word_counts = defaultdict(int)\n",
    "    words = text.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            word_counts[word] += 1\n",
    "\n",
    "    if len(word_counts) == 0:\n",
    "        continue  # Skip completely OOV texts\n",
    "\n",
    "    log_post_0 = log_prior_prob[0] + log_posterior_count_class(word_counts, 0, vocabulary, word_probability_matrix)\n",
    "    log_post_1 = log_prior_prob[1] + log_posterior_count_class(word_counts, 1, vocabulary, word_probability_matrix)\n",
    "\n",
    "    log_confidence_ratio = log_post_0 - log_post_1\n",
    "    confidence_ratio = np.exp(log_confidence_ratio)\n",
    "\n",
    "    uncertain_instances.append((confidence_ratio, text, true_label))\n",
    "\n",
    "# Sort by uncertainty (closest to decision boundary)\n",
    "uncertain_instances.sort(key=lambda x: abs(np.log(x[0])))\n",
    "\n",
    "# Select top 200 most uncertain\n",
    "selected_200 = uncertain_instances[:200]\n",
    "\n",
    "# Extract texts and true labels from the selected instances\n",
    "selected_texts = [x[1] for x in selected_200]\n",
    "selected_labels = [x[2] for x in selected_200]\n",
    "\n",
    "# Combine with original training data from Q1\n",
    "expanded_texts = list(texts) + selected_texts\n",
    "expanded_labels = list(labels) + selected_labels\n",
    "\n",
    "# Rebuild vocabulary and count matrix\n",
    "expanded_vocabulary = set()\n",
    "expanded_word_count_matrix = {0: defaultdict(int), 1: defaultdict(int)}\n",
    "\n",
    "for text, label in zip(expanded_texts, expanded_labels):\n",
    "    for word in text.split():\n",
    "        expanded_word_count_matrix[label][word] += 1\n",
    "        expanded_vocabulary.add(word)\n",
    "\n",
    "expanded_N_vocab = len(expanded_vocabulary)\n",
    "expanded_word_probability_matrix = {0: {}, 1: {}}\n",
    "\n",
    "# Recompute word probabilities with Laplace smoothing\n",
    "for word in expanded_vocabulary:\n",
    "    for label in [0, 1]:\n",
    "        expanded_word_probability_matrix[label][word] = (\n",
    "            expanded_word_count_matrix[label][word] + alpha\n",
    "        ) / (sum(expanded_word_count_matrix[label].values()) + expanded_N_vocab * alpha)\n",
    "\n",
    "# Recompute prior probabilities and the log of them\n",
    "expanded_total = len(expanded_labels)\n",
    "expanded_N_scam = sum(1 for l in expanded_labels if l == 1)\n",
    "expanded_N_nonmal = expanded_total - expanded_N_scam\n",
    "\n",
    "expanded_prior_prob = {0: expanded_N_nonmal / expanded_total, 1: expanded_N_scam / expanded_total}\n",
    "expanded_log_prior_prob = {0: np.log(expanded_prior_prob[0]), 1: np.log(expanded_prior_prob[1])}\n",
    "\n",
    "print(\"Selecting 200 most uncertain instances\\n\")\n",
    "print(f\"New expanded prior probabilies {expanded_prior_prob}\")\n",
    "\n",
    "# Test on original test set\n",
    "validation_predictions = [] \n",
    "for validation_text in unlabelled_validation:\n",
    "    validation_word_counts = defaultdict(int)\n",
    "    for word in validation_text.split():\n",
    "        if word in expanded_vocabulary:\n",
    "            validation_word_counts[word] += 1\n",
    "\n",
    "    if len(validation_word_counts) == 0:\n",
    "        validation_predictions.append(None)\n",
    "        continue\n",
    "\n",
    "    log_0 = expanded_log_prior_prob[0] + log_posterior_count_class(validation_word_counts, 0, expanded_vocabulary, expanded_word_probability_matrix)\n",
    "    log_1 = expanded_log_prior_prob[1] + log_posterior_count_class(validation_word_counts, 1, expanded_vocabulary, expanded_word_probability_matrix)\n",
    "    prediction = 0 if log_0 > log_1 else 1\n",
    "    validation_predictions.append(prediction)\n",
    "\n",
    "# Clean predictions for scoring\n",
    "valid_updated_preds = [p for p in validation_predictions if p is not None]\n",
    "valid_test_labels = [l for p, l in zip(validation_predictions, labels_validation) if p is not None]\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(valid_test_labels, valid_updated_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(valid_test_labels, valid_updated_preds, average=None)\n",
    "conf_matrix = confusion_matrix(valid_test_labels, valid_updated_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Precision (Non-Malicious, Scam): {precision}\")\n",
    "print(f\"Recall (Non-Malicious, Scam): {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set\n",
      "New expanded prior probabilies {0: 0.8045454545454546, 1: 0.19545454545454546}\n",
      "Accuracy: 0.9770\n",
      "Confusion Matrix:\n",
      "[[786  14]\n",
      " [  9 191]]\n",
      "Precision (Non-Malicious, Scam): [0.98867925 0.93170732]\n",
      "Recall (Non-Malicious, Scam): [0.9825 0.955 ]\n",
      "\n",
      "Most probable words in scam class:\n",
      ".: 0.0647\n",
      "!: 0.0274\n",
      ",: 0.0267\n",
      "call: 0.0228\n",
      "£: 0.0151\n",
      "free: 0.0117\n",
      "/: 0.0101\n",
      "&: 0.0101\n",
      "2: 0.0099\n",
      "?: 0.0097\n",
      "\n",
      "Most probable words in non-malicious class:\n",
      ".: 0.0863\n",
      ",: 0.0283\n",
      "?: 0.0273\n",
      "u: 0.0194\n",
      "...: 0.0189\n",
      "!: 0.0186\n",
      "..: 0.0153\n",
      "&: 0.0133\n",
      ";: 0.0133\n",
      "go: 0.0113\n",
      "\n",
      "Most predictive words for non-malicious class:\n",
      "gt: 101.24\n",
      "lt: 100.28\n",
      ":): 89.62\n",
      "ü: 59.58\n",
      "lor: 53.77\n",
      ";: 45.44\n",
      "d: 39.24\n",
      "da: 37.30\n",
      "...: 36.01\n",
      "let: 33.43\n",
      "\n",
      "Most predictive words for scam class:\n",
      "prize: 0.00\n",
      "tone: 0.01\n",
      "select: 0.01\n",
      "paytm: 0.01\n",
      "code: 0.01\n",
      "ringtone: 0.02\n",
      "won: 0.02\n",
      "18: 0.02\n",
      "claim: 0.02\n",
      "mob: 0.02\n",
      "\n",
      "Examples of Scam Classified with High Confidence:\n",
      "R=0.00 | Text: . 4 + call £ - * holiday & urgent 18 t landline 150ppm cash cs await collection po box sae complimentary 10,000 ibiza\n",
      "R=0.00 | Text: . 3 4 + ! call : £ offer * holiday & urgent 18 t landline 150ppm cash cs await collection po box sae tenerife 10,000\n",
      "R=0.00 | Text: . . . , please order text call / : customer tone number [ [ service mobile ] ] colour colour thanks ringtone reference charge 4.50 arrive = red x49 09065989182\n",
      "R=0.00 | Text: . . 2 free u + ! 1st / / wk wk txt tone gr8 hit 150p 16 poly 8007 8007 nokia nokia nokia tones polys\n",
      "R=0.00 | Text: . call £ £ guarantee won customer prize prize claim service 1000 yr 2000 representative cash 10am-7pm\n",
      "\n",
      "Examples of Non-Malicious Classified with High Confidence:\n",
      "R=1476675378795490492860914891215945596928.00 | Text: time : rs. transaction number & & & & & & & & & & ; ; ; ; ; ; ; ; ; ; lt lt lt lt lt # # # gt gt gt gt gt credit account reference decimal\n",
      "R=6655500985701323177560845058048.00 | Text: ? ? ? ? .. .. u u u u , , ... ... ... ... say person yes ! f : hello hello hello o o wen knw knw girl girl mean @ \" \" \" \" t name name g g n d d d d d d lift bt real dat h girlfrnd girlfrnd moral\n",
      "R=132544663841472953542770688.00 | Text: . every & & & & & & ; ; ; ; ; ; lt lt lt # # # gt gt gt big hr\n",
      "R=26424312576786965725184.00 | Text: u , , lor ... ... ... ... food food eat den oso haha well depend mon n la wana okie okie cheap chinese gd ex\n",
      "R=22950856345570626240512.00 | Text: , get like second half & & & & ; ; ; ; lt lt # # gt gt run though almost whole gram gram usually\n",
      "\n",
      "Examples on the Decision Boundary (Uncertain Classification):\n",
      "R=1.00 | Text: . send link picture\n",
      "R=1.02 | Text: . reply glad\n",
      "R=1.07 | Text: . free / give otherwise\n",
      "R=0.89 | Text: . university\n",
      "R=1.16 | Text: . call dear\n",
      "\n",
      "Amount of test instance with R-value between 0.8 and 1.2 (using Q1 model): 8\n",
      "Amount of test instance with R-value between 0.8 and 1.2 (using Q3 model): 6\n"
     ]
    }
   ],
   "source": [
    "for text, label in zip(expanded_texts, expanded_labels):\n",
    "    for word in text.split():\n",
    "        expanded_word_count_matrix[label][word] += 1\n",
    "        expanded_vocabulary.add(word)\n",
    "\n",
    "expanded_N_vocab = len(expanded_vocabulary)\n",
    "expanded_word_probability_matrix = {0: {}, 1: {}}\n",
    "\n",
    "# Recompute word probabilities with Laplace smoothing\n",
    "for word in expanded_vocabulary:\n",
    "    for label in [0, 1]:\n",
    "        expanded_word_probability_matrix[label][word] = (\n",
    "            expanded_word_count_matrix[label][word] + alpha\n",
    "        ) / (sum(expanded_word_count_matrix[label].values()) + expanded_N_vocab * alpha)\n",
    "\n",
    "# Recompute prior probabilities and the log of them\n",
    "expanded_total = len(expanded_labels)\n",
    "expanded_N_scam = sum(1 for l in expanded_labels if l == 1)\n",
    "expanded_N_nonmal = expanded_total - expanded_N_scam\n",
    "\n",
    "expanded_prior_prob = {0: expanded_N_nonmal / expanded_total, 1: expanded_N_scam / expanded_total}\n",
    "expanded_log_prior_prob = {0: np.log(expanded_prior_prob[0]), 1: np.log(expanded_prior_prob[1])}\n",
    "\n",
    "print(\"Evaluation on test set\")\n",
    "print(f\"New expanded prior probabilies {expanded_prior_prob}\")\n",
    "\n",
    "# Test on original test set\n",
    "updated_predictions = []\n",
    "updated_conf_ratios = [] \n",
    "for test_text in test_texts:\n",
    "    test_word_counts = defaultdict(int)\n",
    "    for word in test_text.split():\n",
    "        if word in expanded_vocabulary:\n",
    "            test_word_counts[word] += 1\n",
    "\n",
    "    if len(test_word_counts) == 0:\n",
    "        updated_predictions.append(None)\n",
    "        continue\n",
    "\n",
    "    log_0 = expanded_log_prior_prob[0] + log_posterior_count_class(test_word_counts, 0, expanded_vocabulary, expanded_word_probability_matrix)\n",
    "    log_1 = expanded_log_prior_prob[1] + log_posterior_count_class(test_word_counts, 1, expanded_vocabulary, expanded_word_probability_matrix)\n",
    "    \n",
    "    prediction = 0 if log_0 > log_1 else 1\n",
    "    updated_predictions.append(prediction)\n",
    "    \n",
    "    log_conf_ratio = log_0 - log_1\n",
    "    conf_ratio = np.exp(log_conf_ratio)\n",
    "    updated_conf_ratios.append((test_text, prediction, conf_ratio))\n",
    "\n",
    "# Clean predictions for scoring\n",
    "valid_updated_preds = [p for p in updated_predictions if p is not None]\n",
    "valid_test_labels = [l for p, l in zip(updated_predictions, test_labels) if p is not None]\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(valid_test_labels, valid_updated_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(valid_test_labels, valid_updated_preds, average=None)\n",
    "conf_matrix = confusion_matrix(valid_test_labels, valid_updated_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Precision (Non-Malicious, Scam): {precision}\")\n",
    "print(f\"Recall (Non-Malicious, Scam): {recall}\")\n",
    "\n",
    "# Most probable words after semi-supervised learning\n",
    "top_scam_words = sorted(expanded_word_probability_matrix[1].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "top_nonmal_words = sorted(expanded_word_probability_matrix[0].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"\\nMost probable words in scam class:\")\n",
    "for word, prob in top_scam_words:\n",
    "    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\nMost probable words in non-malicious class:\")\n",
    "for word, prob in top_nonmal_words:\n",
    "    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "# Most predictive words after semi-supervised learning\n",
    "ratios = {\n",
    "    word: expanded_word_probability_matrix[0][word] / expanded_word_probability_matrix[1][word]\n",
    "    for word in expanded_vocabulary if expanded_word_probability_matrix[1][word] > 0\n",
    "}\n",
    "\n",
    "top_predictive_nonmal = sorted(ratios.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "top_predictive_scam = sorted(ratios.items(), key=lambda x: x[1])[:10]\n",
    "\n",
    "print(\"\\nMost predictive words for non-malicious class:\")\n",
    "for word, ratio in top_predictive_nonmal:\n",
    "    print(f\"{word}: {ratio:.2f}\")\n",
    "\n",
    "print(\"\\nMost predictive words for scam class:\")\n",
    "for word, ratio in top_predictive_scam:\n",
    "    print(f\"{word}: {ratio:.2f}\")\n",
    "\n",
    "# Sort and print results\n",
    "high_conf_ratio_scam = sorted(updated_conf_ratios, key=lambda x: x[2])[:5]\n",
    "low_conf_ratio_nonmal = sorted(updated_conf_ratios, key=lambda x: x[2], reverse=True)[:5]\n",
    "boundary_cases = sorted(updated_conf_ratios, key=lambda x: abs(np.log(x[2])))[:5]\n",
    "\n",
    "# Print results\n",
    "print(\"\\nExamples of Scam Classified with High Confidence:\")\n",
    "for text, label, R in high_conf_ratio_scam:\n",
    "    print(f\"R={R:.2f} | Text: {text}\")\n",
    "\n",
    "print(\"\\nExamples of Non-Malicious Classified with High Confidence:\")\n",
    "for text, label, R in low_conf_ratio_nonmal:\n",
    "    print(f\"R={R:.2f} | Text: {text}\")\n",
    "\n",
    "print(\"\\nExamples on the Decision Boundary (Uncertain Classification):\")\n",
    "for text, label, R in boundary_cases:\n",
    "    print(f\"R={R:.2f} | Text: {text}\")\n",
    "\n",
    "# Filter instances where confidence ratio of Q1 model is between 0.8 and 1.2\n",
    "mid_confidence = [x for x in confidence_ratios if 0.8 <= x[2] <= 1.2]\n",
    "print(f\"\\nAmount of test instance with R-value between 0.8 and 1.2 (using Q1 model): {len(mid_confidence)}\")\n",
    "\n",
    "# Filter instances where confidence ratio of Q3 model is between 0.8 and 1.2\n",
    "updated_mid_confidence = [x for x in updated_conf_ratios if 0.8 <= x[2] <= 1.2]\n",
    "print(f\"Amount of test instance with R-value between 0.8 and 1.2 (using Q3 model): {len(updated_mid_confidence)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
